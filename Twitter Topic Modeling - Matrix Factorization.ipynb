{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Topic Modeling with Non-negative Matrix Factorization\n",
    "\n",
    "###### Ignacio Antequera Sanchez\n",
    "\n",
    "*Content/Trigger Warning: This project uses real-world data from Twitter, but that comes at the risk of our dataset containing tweets about sensitive or triggering topics. This project should be approached without having to dig into the tweets present and reading about content*\n",
    "\n",
    "# 0. Introduction\n",
    "---\n",
    "Hello Everyone!\n",
    "\n",
    "My name is Ignacio Antequera Sanchez and in this project I will use the techniques for recommender systems in an unexpected way to help us model topics found on Twitter. Throughout this project, I will practice extracting topics from tweets using matrix factorization. This method assumes every tweet is a combination of several topics weighted by their prevailance in the text. This approach in fact finds a low-dimensional representation of the tweets (through the topic weights).\n",
    "\n",
    "For this project, we will be working with tweets about the pandemic 2020 when the pandemic entered our lives. The dataset is obtained from [Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april?select=2020-04-30+Coronavirus+Tweets.CSV) and the preprocessing we have done followed the steps [here](https://www.kaggle.com/satanizer/covid-19-tweets-analysis). For computational speed we will first analyze a dataset from one day: April 30, 2020. I encourage you to explore this dataset further and see how topics change over time. I might include some additional analysis from other different days in the future and study how topics change over time\n",
    "\n",
    "Without further ado, let's delve into the data!\n",
    "\n",
    "---\n",
    "\n",
    "# 1. The Data\n",
    "---\n",
    "*Extracted from [Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april?select=2020-04-30+Coronavirus+Tweets.CSV)\n",
    "\n",
    "First let's read the dataset into a data frame and have a look what is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('tweets-2020-4-30-1.csv')\n",
    "text = text.fillna('') # some rows are nan so replace with empty string\n",
    "text.tail()\n",
    "\n",
    "np.random.seed(416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Some preprocessing\n",
    "\n",
    "The dataset you have just loaded was actually pre-processed by me in a different project. Here I briefly describe the steps handled already just so you know that there are usually some extra things that need to be done with text data. The code is also presented below.\n",
    "\n",
    "* Removed tweets not in English. This is a tricky modeling choice, but one that is pretty common for simplicity and accuracy. Like when discussing bias, a better choice would probably to build up separate models for each language. \n",
    "* Removed URLs from tweets (not relevant to analysis)\n",
    "* Make all text lower-case\n",
    "* Remove all punctuation\n",
    "* Remove stop-words (e.g., \"a\", \"the\", \"to\") using [NLTK](https://www.nltk.org/).\n",
    "* Also remove some too frequent terms related to COVID that end up skewing the analysis.\n",
    "\n",
    "The code for these steps is shown below. The original dataset had extra columns other than just text.\n",
    "\n",
    "```\n",
    "# select tweets in English\n",
    "text = data['text'][data['lang']=='en']\n",
    "\n",
    "# remove URL links\n",
    "text = text.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
    "\n",
    "# make lower case\n",
    "text = text.str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "text = text.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# remove stopwords and common COVID terms\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['#coronavirus', '#coronavirusoutbreak', \n",
    "                   '#coronavirusPandemic', '#covid19', '#covid_19', \n",
    "                   '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', \n",
    "                   'covid19','covid-19', 'covid„Éº19'])\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)  # Trick to make string separated by spaces\n",
    "\n",
    "text = text.apply(remove_stopwords)\n",
    "```\n",
    "\n",
    "## TF-IDF Matrix\n",
    "\n",
    "Remember that matrix factorization methods work on matrices of numbers not text so we need to convert the text into a meaningful numeric representation.\n",
    "\n",
    "Frequency-Inverse Document Frequency is a good way to do this since it defines a word weight vector for each document by accounting for the most popular words such as `the` or `a`.  We can extract it using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (119147, 183012)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
    "tf_idf = vectorizer.fit_transform(text['text'])\n",
    "\n",
    "# extract feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# check out the shape\n",
    "print(\"TF-IDF matrix shape:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Question 1** Counts\n",
    "\n",
    "Make two variables `num_tweets` and `num_words` that store the number of tweets in our dataset and number of words in our analysis respectively. Use the number of words as the number of features after doing the TF-IDF computation. Check the sense of shape object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute num_tweets and num_words\n",
    "num_tweets = tf_idf.shape[0]\n",
    "num_words = tf_idf.shape[1]\n",
    "\n",
    "print(num_tweets)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Tweets with Topics\n",
    "\n",
    "We will use a particular technique similar to matrix factorization for recommendation to help us model tweets. In particular we will use a model called Non-negative Matrix Decomposition to help us discover topics.\n",
    "\n",
    "You might be wondering how we can use an approach we taught for recommender systems to model tweets, when there is no notion of recommending a tweet. The idea is to try to create two matrices to describe \"Tweet factors\" and \"Word factors\" that will hopefully correspond to distinct topics of discussion. Just like with matrix factorization for recommendation, our hopse is that each factor corresponds to a topic.\n",
    "\n",
    "### üîç **Question 2** NMF\n",
    "\n",
    "We will use the [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) method from `scikit-learn` to extract the topics. \n",
    "\n",
    "Set up an NMF model with 5 components and fit it to our TF-IDF data. Use the `fit_transform` as shown in the example for the documentation above to both fit the NMF model and transform our tweet data in one step.\n",
    "\n",
    "When creating the model, you will want to use the following hyperparameters to ensure you get the same results as us:\n",
    "* `init='nndsvd'`\n",
    "* `n_components=5`\n",
    "\n",
    "When fitting the model, we will fit it on our TF-IDF data which is a matrix of the shape `(num_tweets, num_words)`.\n",
    "\n",
    "Save your model in a variable called `nmf` and the projected tweets in a variable called `tweets_projected`. Check the function fit_transform of the [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html), applied to tf_idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# TODO create and fit the model and transform our data\n",
    "nmf = NMF(n_components=5, init='nndsvd')\n",
    "tweets_projected = nmf.fit_transform(tf_idf)\n",
    "\n",
    "# This will create an NMF model with 5 components, initialize it using the \"nndsvd\" method, fit it to the TF-IDF data, \n",
    "# and transform the data to produce the projected tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Question 3** Inspecting Components\n",
    "\n",
    "The topics are stored within the object `nmf.components_`. Investigate this matrix and the `tweets_projected` matrices by printing their values and their shapes. Make sure you undertsand why each one has the shape it does.\n",
    "\n",
    "Looking at the `nmf.components_` field, does it correspond to the \"Tweet factors\" or \"Word factors\" in the terminology of matrix factorization? Save your answer as a string in a variable called `q3`. \n",
    "\n",
    "* If you think the answer is Tweet factors, write `q3 = 'tweet'`\n",
    "* If you think the answer is Word factors, write `  q3 = 'word'`\n",
    "\n",
    "This is a question you can write your answer to without needing to write a computation in code. You will want to print out the data or the shape of the data to help you make your determination though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define q3 = ...\n",
    "# Print the shapes of nmf.components_ and tweets_projected\n",
    "print(\"Shape of nmf.components_:\", nmf.components_.shape)\n",
    "print(\"Shape of tweets_projected:\", tweets_projected.shape)\n",
    "\n",
    "# Determine whether nmf.components_ corresponds to \"Tweet factors\" or \"Word factors\"\n",
    "q3 = 'word' # Based on the description, nmf.components_ corresponds to \"Word factors\"\n",
    "print(\"The answer to question 3 is:\", q3)\n",
    "\n",
    "# The answer is 'word' because the nmf.components_ matrix corresponds to the \"Word factors\" in the terminology of matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Topics\n",
    "\n",
    "We are now interested in inspecting each topic to find the most prevelant or meaningful words for that topic. We'll consider the words with the highest weights for a topic in NMF model to be the most important words for that topic. Recall that the words themselves are stored in a variable called `feature_names`.\n",
    "\n",
    "### üîç **Question 4** Small Example\n",
    "Before trying to investigate the values in the real data, let's do a small example first to explore how this can be done. You can use the [`argsort()`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) to get a list of array indices sorted by the values at those indices; this is useful when you want to use the ordered indices for another purpose. \n",
    "\n",
    "In the cell below, we have defined variables `small_words` and `small_weights` that correspond to a made-up example of words with weights from a single topic in NMF. To be specific, the word at index `i` in `small_words` will have weight `small_weights[i]`. You should write code in the cell below to make a new variable `sorted_small_words` that stores a `list` of the words of `small_words` but in sorted order from *largest weight* to *smallest weight*.\n",
    "\n",
    "Consider numpy function [take_along_axis](https://numpy.org/doc/stable/reference/generated/numpy.take_along_axis.html)\n",
    "\n",
    "\n",
    "*Notes*:\n",
    "* Pay special attention to the sort order specified by `argsort`. If you need to reverse an `numpy` array stored in a variable `a`, you can write `a[::-1]`. \n",
    "* Like normal, you should not hard code the answer to this problem but write code to make it work (even though you could easily do this example by just writing out the values). We want you to practice this now since in the next few problems you will have to do this on the real-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = ['dogs', 'cats', 'axolotl']\n",
    "small_weights = np.array([1, 4, 2])\n",
    "\n",
    "#TODO Write code to make sorted_small_words as described above\n",
    "\n",
    "# sort the indices based on weights\n",
    "ind = np.argsort(small_weights)[::-1]\n",
    "\n",
    "# use take_along_axis to get the sorted words array\n",
    "sorted_small_words = np.take_along_axis(np.array(small_words), ind, axis=None).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Question 5** Words from Topic\n",
    "We will now generalize the code you wrote for the last section to work on our real dataset.\n",
    "\n",
    "Write a function `words_from_topic` to extract an ordered list of words in a topic (highest weight first). Please see the documentation provided in the starter function to see a description of the parameters and return.\n",
    "\n",
    "*Note*: Your solution should look very similar to the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add code similar to the previous question for sorted_feature_names and return\n",
    "\n",
    "def words_from_topic(topic, feature_names):\n",
    "  \"\"\"\n",
    "  Sorts the words by their weight in the given topic from largest to smallest.\n",
    "  topic and feature_names should have the same number of entries.\n",
    "\n",
    "  Args:\n",
    "  - topic (np.array): A numpy array with one entry per word that shows the weight in this topic.\n",
    "  - feature_names (list): A list of words that each entry in topic corresponds to\n",
    "\n",
    "  Returns:\n",
    "  - A list of words in feature_names sorted by weight in topic from largest to smallest. \n",
    "  \"\"\"\n",
    "\n",
    "  # TODO implement this function\n",
    "  ind = np.argsort(topic)[::-1]\n",
    "  sorted_feature_names = np.array(feature_names)[ind]\n",
    "  return sorted_feature_names.tolist()\n",
    "\n",
    "  # We use np.argsort() to obtain the indices of the words sorted by weight in the topic, but we reverse the order with [::-1] to sort them in descending order\n",
    "  # We then use np.take_along_axis() to extract the corresponding words from feature_names.\n",
    "  # Finally, we convert the resulting numpy array to a list using tolist()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented the function above,  you should be able to run the cell below that uses your function to print out the top 10 words in each topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(components, feature_names, n_top_words):\n",
    "    \"\"\" \n",
    "    print_top_words prints the first n_top_words for each topic in components\n",
    "    \"\"\"\n",
    "    for topic_index, topic in enumerate(components):\n",
    "        ordered_words = words_from_topic(topic, feature_names)\n",
    "        top_words = ', '.join(ordered_words[:n_top_words])\n",
    "        print(f'Topic: #{topic_index}: {top_words}')\n",
    "\n",
    "print_top_words(nmf.components_, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Tweet\n",
    "Next let's look at a specific tweet (index 40151) and the individual contributions of the topics. The cell below prints the text of the original tweet and then the value of the tweet after being transformed by our NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 40151\n",
    "print(text.iloc[index]['text'])\n",
    "print(tweets_projected[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Question 6** Looking at a tweet\n",
    "\n",
    "Looking at the topic values for the tweet above, which topic (Topic #0 to Topic #4) is it most  associated with? Save your answer in a variable called `q6`.\n",
    "\n",
    "For this problem, you can hard-code your answer as a number. For example, if you look at the result and believe it is most associated with Topic 0, you could write `q6 = 0`.\n",
    "\n",
    "Does this tweet make sense to be grouped in a topic with the words shown in the topic word lists you printed in the last problem?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look at the output above to identify which topic the tweet above is most associated to\n",
    "q6 = 2\n",
    "\n",
    "# The tweet is most associated with Topic #2 since it has the highest value. So we can set q6 = 2.\n",
    "# The tweet contains words that are related to Topic #2 so it does make sense for the tweet to be grouped in this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Question 7** Largest Topic\n",
    "In our analysis above where we modeled each tweet in 5 topics, which topic has the most tweets strongly associated with it? \n",
    "\n",
    "For each tweet, calculate which topic it is most strongly associated with by looking at the topic values for the tweet. If there is ever a tie for the largest topic weight, take the one with the lowest index (although this is unlikely to happen in our dataset).\n",
    "\n",
    "Save the index of the topic with the most tweets strongly associated with it in a variable called `largest_topic`. The result should be an integer for the index of the largest topic.\n",
    "\n",
    "*Hint: There is a very efficient way to do this using code like we wrote in HW6, but there are many ways to solve this problem in general*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find index of largest topic\n",
    "y = np.zeros(len(tweets_projected))\n",
    "for i in range(len(tweets_projected)):\n",
    "    y[i] = np.argmax(tweets_projected[i]) # argmax() function of numpy to find the index of the largest element in each row of tweets_projected.\n",
    "count = np.unique(y, return_counts = True)\n",
    "for i in range(np.shape(count)[1]):\n",
    "    if count[1][i] == max(count[1]):\n",
    "        largest_topic = int(count[0][i])\n",
    "\n",
    "print(largest_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means that the topic number 4 has the most tweets strongly associated with it in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Trends\n",
    "\n",
    "One benefit of using matrix factorization to a small dimension, is it lets us visualize tweets in this \"topic space\" to find any interesting groupings. \n",
    "\n",
    "Now in our earlier analysis, we modeled each tweet as 5 topics but that is hard to visualize. \n",
    "\n",
    "In the next cell, make a new NMF model and projected tweets (called `nmf_small` and `tweets_projected_small` respectively) with 3 components instead of 5. Use the same settings for the other parameters as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_small = NMF(n_components=3, init='nndsvd')\n",
    "tweets_projected_small = nmf_small.fit_transform(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the topics in this small model. Unsurprisingly the seem mostly the same but a couple topics had to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(nmf_small.components_, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 3 values for each tweet, we can actually plot each tweet in 3D space to see how all the tweets relate to each other. The following cell does exactly that. You don't need to understand all the specifics of how to make a 3D plot, but just note it is using the 3 topic values for each tweet as the x, y, z coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up axes to plot on\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Make 3D scatterplot\n",
    "ax.scatter(tweets_projected_small[:, 0], tweets_projected_small[:, 1], tweets_projected_small[:, 2])\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Topic 0')\n",
    "ax.set_ylabel('Topic 1')\n",
    "ax.set_zlabel('Topic 2')\n",
    "\n",
    "# Rotate plot to be easily viewed\n",
    "ax.view_init(30, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, it looks like there is a small cluster of Tweets that are far away from all the others when looking at Topic 2. In other words, there are a few tweets that are very far in the Topic 2 direction while the majority of tweets are spread out more in Topic 0/1.\n",
    "\n",
    "### üîç **Question 8** Outlier Tweets (Optional)\n",
    "Let's look into the tweets that seem very different than the rest. \n",
    "\n",
    "For this problem, we want you to compute all of the unique tweets (since there are some duplicates) that appear in this region in the \"topic space\". \n",
    "\n",
    "Below, we explain the steps to do this computation. Save your result in a variable called `outlier_tweets` that has type `numpy.array` and stores all the unique tweets that are these outliers (as described below).\n",
    "\n",
    "For this problem, you should follow these steps:\n",
    "1. Find the which rows in our `tweets_projected_small` our outliers. We will define this tweets as ones that have a value of `0.15` or more for Topic 2.\n",
    "2. Now that we know which rows are outliers, use that information to access the `text` column of our original tweets `DataFrame` `text` for those rows.\n",
    "3. Use the `.unique` function available on a column of a `pandas` `DataFrame`to find all the unique values.\n",
    "\n",
    "If you follow these steps (particularly the last), you will end up with a `numpy.array` with all of the unique tweets that meet this criteria. Note that many of the tweets look similar, but they count as unique tweets since they have some character differences!\n",
    "\n",
    "Do you spot a theme amongst these tweets? Do you think there is an explanation why our model isolated them as their own topic?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement the process explained above\n",
    "z = y = np.zeros(len(tweets_projected_small[:, 2]))\n",
    "#fill in z with index for not small projected tweet and -1 otherwise\n",
    "z = np.where(tweets_projected_small[:, 2] >= 0.15, y, -1)\n",
    "# This sets the value of each element in z to the corresponding index of y if the corresponding element in tweets_projected_small[:, 2] is greater than or equal to 0.15, and to -1 otherwise.\n",
    "\n",
    "rows = np.delete(np.unique(z, return_counts=True)[0],0)\n",
    "outlier_tweets = np.empty(len(rows), dtype=object)\n",
    "for i in range(len(rows)):\n",
    "    outlier_tweets[i] = text.iloc[int(rows[i])]['text']\n",
    "outlier_tweets = np.unique(outlier_tweets, return_counts=True)[0]\n",
    "\n",
    "print(outlier_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output provided, it seems like the theme of the outlier tweets is about a virtual meeting or event that will take place on April 30th at 5pm UK time. \n",
    "It is possible that the model isolated these tweets as their own topic because they contain unique wordsor phrases that are not present in the other tweets in the dataset\n",
    "\n",
    "It is also possible that these tweets were grouped together because they contain a higher frequency of certain keywords that the model identified as important for this particular topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
